{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea0431c-bc35-46d2-8f67-8194807a2a1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline: Bronze to Silver\n",
    "\n",
    "## Data Source\n",
    "\n",
    "- **Catalog Location:**  `workspace.hospital_bronze.visits`\n",
    "- **Format:** Delta Lake Table\n",
    "\n",
    "\n",
    "## Destination\n",
    "\n",
    "- **Catalog Location:** `workspace.hospital_silver.visits`\n",
    "- **Format:** Delta Lake Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48d84bec-64f5-4010-ad73-6731d5a39685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "entity = \"visits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff1619a-59d8-41b3-9c49-102fe8fd4951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Storage\n",
    "catalog_name = \"workspace\"\n",
    "schema_silver = \"hospital_silver\"\n",
    "schema_bronze = \"hospital_bronze\"\n",
    "schema_gold = \"hospital_gold\"\n",
    "\n",
    "# data source path\n",
    "data_source = \"s3://buckethospitaldata/data_batching/\"\n",
    "\n",
    "# for streaming: schema and checkpoint location (stored in data source S3 buckets)\n",
    "checkpoint_location = f\"s3://buckethospitaldata/pipeline_checkpoints/data_streaming/_checkpoints/silver/{entity}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722c0b86-c40b-407b-9a44-baa745040006",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Data to from Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3184c34a-9f9a-46c3-ae60-2ffaf0137d3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.table(f\"{catalog_name}.{schema_bronze}.{entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b1f555-56e6-482b-9b71-5864151b77b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bb268d-6104-483b-93d6-3a237dcc2f0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Matching Data Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2573dc9-8ce7-4cd2-b80e-c1bb28114637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Cost and insurance columns: float\n",
    "- Room_Charges_daily_rate: float\n",
    "- Follow_Up_Visit_Date: date\n",
    "- Admitted_Date: date\n",
    "- Discharge_Date: date\n",
    "- Emergency_visit: Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d775846f-faac-43db-a403-1289aac95052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, when\n",
    "\n",
    "df = df.withColumn(\"Room_Charges_daily_rate\", col(\"Room_Charges_daily_rate\").cast(\"float\")) \\\n",
    "        .withColumn(\"Treatment_Cost\", col(\"Treatment_Cost\").cast(\"float\")) \\\n",
    "        .withColumn(\"Medication_Cost\", col(\"Medication_Cost\").cast(\"float\")) \\\n",
    "        .withColumn(\"Insurance_Coverage\", col(\"Insurance_Coverage\").cast(\"float\")) \\\n",
    "        .withColumn(\"Date_of_Visit\", to_date(col(\"Date_of_Visit\"), \"M/d/yyyy\")) \\\n",
    "        .withColumn(\"Follow_Up_Visit_Date\", to_date(col(\"Follow_Up_Visit_Date\"), \"M/d/yyyy\")) \\\n",
    "        .withColumn(\"Admitted_Date\", to_date(col(\"Admitted_Date\"), \"M/d/yyyy\")) \\\n",
    "        .withColumn(\"Discharge_Date\", to_date(col(\"Discharge_Date\"), \"M/d/yyyy\")) \\\n",
    "        .withColumn(\"Emergency_Visit\", when(col(\"Emergency_Visit\") == \"Yes\", True)\n",
    "                                        .when(col(\"Emergency_Visit\") == \"No\", False)\n",
    "                                        .otherwise(None))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d44513-ca6c-4ab7-86d2-186a2e0f9312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da34242-f783-41e6-be95-ea641a62ffd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, when\n",
    "\n",
    "# Fill Room_Type with 'Outpatient' if missing\n",
    "df = df.fillna({'Room_Type': 'Outpatient'})\n",
    "\n",
    "# Fill Insurance_Coverage with 0.0 if missing means no coverage\n",
    "df = df.fillna({'Insurance_Coverage': 0.0})\n",
    "\n",
    "# For Admitted_Date and Discharge_Date, fill with None or leave as is if missingness is meaningful\n",
    "# Flag column: patients who are admitted\n",
    "df = df.withColumn('Was_Admitted', when(df.Admitted_Date.isNull(), False).otherwise(True))\n",
    "# Flag column: patients who follow up\n",
    "df = df.withColumn(\n",
    "    \"Has_Follow_Up\",\n",
    "    when(col(\"Follow_Up_Visit_Date\").isNotNull(), True).otherwise(False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5bc4f83-324a-42db-a9db-9364322b2f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Calculate Derived Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f540d1-a2f9-45dc-8594-22f6773dff5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp, col, concat_ws, datediff, when, sha2\n",
    "\n",
    "\n",
    "# to be deleted\n",
    "# df = spark.read.table(f\"{catalog_name}.{schema_bronze}.{entity}\")\n",
    "\n",
    "# Length of Stay\n",
    "df = df.withColumn(\n",
    "    \"Length_of_stay\",\n",
    "    when(\n",
    "        col(\"Discharge_Date\").isNull() | col(\"Admitted_Date\").isNull(),\n",
    "        0\n",
    "    ).otherwise(\n",
    "        datediff(col(\"Discharge_Date\"), col(\"Admitted_Date\"))\n",
    "    )\n",
    ")\n",
    "           \n",
    "df = (df.withColumn(\"Room_Cost\", col(\"Room_Charges_daily_rate\") * col(\"Length_of_stay\"))\n",
    "        .withColumn(\"Visit_Cost\", col(\"Room_Cost\") + col(\"Medication_Cost\") + col(\"Treatment_Cost\") - col(\"Insurance_Coverage\"))\n",
    "        .withColumn(\"Revenue_per_visit\", col(\"Room_Cost\") + col(\"Medication_Cost\") + col(\"Treatment_Cost\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56e620d-98e2-4eeb-865a-341e98584bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write Data to Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2f6b3b-2ca3-40e5-bdf9-9360320c1990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpoint_location)\n",
    "        .option(\"mergeSchema\", \"true\")  # Optional but useful\n",
    "        .outputMode(\"append\")\n",
    "        .trigger(once=True)\n",
    "        .table(f\"{catalog_name}.{schema_silver}.{entity}\")\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_bronze_to_silver_visits",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "data_engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
