{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ebe1fee-63ec-4e01-aaa8-8d73e6a1aad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline: Bronze to Silver\n",
    "\n",
    "## Data Source\n",
    "\n",
    "- **Catalog Location:**  `workspace.hospital_bronze.patients`\n",
    "- **Format:** Delta Lake Table\n",
    "\n",
    "\n",
    "## Destination\n",
    "\n",
    "- **Catalog Location:** `workspace.hospital_silver.patients`\n",
    "- **Format:** Delta Lake Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a39989d6-1f04-4ebe-b386-78668fe93988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "entity = \"patients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23b3f2f-0dbb-420c-a7d0-1c169961350d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Storage\n",
    "catalog_name = \"workspace\"\n",
    "schema_silver = \"hospital_silver\"\n",
    "schema_bronze = \"hospital_bronze\"\n",
    "schema_gold = \"hospital_gold\"\n",
    "\n",
    "# data source path\n",
    "data_source = \"s3://buckethospitaldata/data_batching/\"\n",
    "\n",
    "# for streaming: schema and checkpoint location (stored in data source S3 buckets)\n",
    "checkpoint_location = f\"s3://buckethospitaldata/pipeline_checkpoints/data_streaming/_checkpoints/silver/{entity}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1377009-6393-442c-abab-459bba6cac8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Data to from Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446eef04-55d5-4b4c-9742-62233485bedd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.readStream.table(f\"{catalog_name}.{schema_bronze}.{entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0006bfa3-586b-4796-8bea-192f60db766a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Convert Data format: Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e1b6bf-cc40-46e8-aa15-336c77583254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, to_date, when\n",
    "\n",
    "# df = df.withColumn(\"Gender\", when(col(\"Gender\") == \"Male\", 1)\n",
    "#                                         .when(col(\"Gender\") == \"Female\", 0)\n",
    "#                                         .otherwise(None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb0fcfdb-5432-48ee-8f7b-8b2db030096a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3250c612-2b1d-44bc-9a28-f3d27dd4c152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "\n",
    "def count_missing_values(df, sort=True, as_pandas=True):\n",
    "    \"\"\"\n",
    "    Counts missing values (nulls for all columns, and NaNs for float/double columns) in a PySpark DataFrame,\n",
    "    and shows the percentage of missing values per column.\n",
    "    \n",
    "    Parameters:\n",
    "        df (DataFrame): The PySpark DataFrame to check.\n",
    "        sort (bool): Whether to sort the result by missing count (descending).\n",
    "        as_pandas (bool): Whether to return the result as a pandas DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame or dict: Missing value counts and percentages per column.\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    float_cols = [c for c, t in df.dtypes if t in ['float', 'double']]\n",
    "    other_cols = [c for c in df.columns if c not in float_cols]\n",
    "\n",
    "    exprs = [count(when(col(c).isNull() | isnan(col(c)), c)).alias(c) for c in float_cols] + \\\n",
    "            [count(when(col(c).isNull(), c)).alias(c) for c in other_cols]\n",
    "\n",
    "    result_row = df.select(exprs).collect()[0].asDict()\n",
    "    \n",
    "    # Prepare results with percentage\n",
    "    results = []\n",
    "    for col_name, missing_count in result_row.items():\n",
    "        percent = (missing_count / total_rows * 100) if total_rows > 0 else 0\n",
    "        results.append((col_name, missing_count, round(percent, 2)))\n",
    "    \n",
    "    if as_pandas:\n",
    "        import pandas as pd\n",
    "        result_df = pd.DataFrame(results, columns=['column', 'missing_count', 'missing_percent'])\n",
    "        if sort:\n",
    "            result_df = result_df.sort_values('missing_count', ascending=False).reset_index(drop=True)\n",
    "        return result_df\n",
    "    else:\n",
    "        if sort:\n",
    "            results = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "        return results\n",
    "\n",
    "\n",
    "# df_batch = spark.read.table(f\"{catalog_name}.{schema_bronze}.{entity}\")\n",
    "\n",
    "# missing_counts = count_missing_values(df_batch)\n",
    "# print(df_batch.count())\n",
    "# print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51331ed8-2c11-4f9b-9926-bccf1e2ba56a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write Data to Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3c31dd5-bf3c-4abb-a5ca-6a302a733a11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\n",
    "    .option(\"mergeSchema\", \"true\")  # Optional but useful\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True)\n",
    "    .table(f\"{schema_silver}.{entity}\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pipeline_bronze_to_silver_patients",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
